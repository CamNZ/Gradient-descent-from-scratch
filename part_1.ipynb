{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent from scratch - part 1, simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Gradient descent is a very commonly used optimization algorithm in modern machine learning. This two part series is intended to help people gain a better understanding of how it works by implementing it without the use of any machine learning libraries. In part one we will use gradient descent to solve a trivial linear regression problem to introduce the concepts. In part two we will build on these concepts to train a handwritten digit classifier. Feel free to skip straight to part two if you want. The format will be short explanations, followed by python code you can run from within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "Simple linear regression is a statistical method that models the relationship between two variables. It attempts to predict as accurately as possible an independent variable (usually plotted on the y axis of a cartesian plane) from a dependent variable (usually plotted on the x axis). This relationship between dependent and independent variable is modelled by: \n",
    "\n",
    "$ y = mx + b $\n",
    "\n",
    "where *y*, the output, is predicted by multiplying the input *x* by *m* and adding *b*. Optimal values for *m* and *b* are usually calculated using the ordinary least squares method. Instead we are going to use gradient descent to find (close to) optimal values for *m* and *b* to help further our understanding of gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swedish Auto Insurance Dataset\n",
    "The Swedish Auto Insurance Dataset involves predicting the total payment for all claims in thousands of Swedish Kronor, given the total number of claims. It was produced by the Swedish Committee on Analysis of Risk Premium in Motor Insurance. [More information](http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr06.html). Lets load up the dataset and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np # python array library\n",
    "import random\n",
    "import matplotlib.pyplot as plt # python library for producing figures and plots\n",
    "%matplotlib inline\n",
    "\n",
    "# load dataset\n",
    "dataset = np.genfromtxt('swedish_auto.csv', delimiter=',')\n",
    "claims = dataset[:, 0] # number of claims (independent variable)\n",
    "payments = dataset[:, 1] # total payment (dependent variable)\n",
    "\n",
    "# plot dataset\n",
    "fig_1 = plt.figure(1)\n",
    "plt.scatter(payments, claims, color='blue')\n",
    "plt.title('Swedish auto insurance dataset\\nRaw data')\n",
    "plt.xlabel('Number of claims')\n",
    "plt.ylabel('Total payment for all claims (thousand Kronor)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see visually there is a clear pattern to the distribution of this data. Simple linear regression is likely to fit it well.\n",
    "\n",
    "First we will quickly find an optimal solution for m and b using the ordinary least squares method to see what we are aiming to replicate with gradient descent. Feel free to skip over the details of the least squares method. If you are curious you can read more [here](https://en.wikipedia.org/wiki/Ordinary_least_squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinary_least_squares(x_set, y_set):\n",
    "    \n",
    "    numerator = sum(( (x - np.mean(x_set)) * (y - np.mean(y_set)) for x, y in zip(x_set, y_set) ))\n",
    "    denominator = sum(( (x - np.mean(x_set))**2 for x in x_set ))\n",
    "\n",
    "    m = numerator / denominator\n",
    "    b = np.mean(y_set) - m * np.mean(x_set)\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = ordinary_least_squares(claims, payments)\n",
    "pred = [m * x + b for x in claims]\n",
    "\n",
    "# plot\n",
    "fig_2 = plt.figure(2)\n",
    "plt.scatter(payments, claims, color='blue')\n",
    "plt.plot(pred, claims, color='red')\n",
    "plt.title('Swedish auto insurance dataset\\nSimple linear regression')\n",
    "\n",
    "plt.xlabel('Number of claims')\n",
    "plt.ylabel('Total payment for all claims (thousand Kronor)')\n",
    "plt.legend(['Least squares regression'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('m: {0}, b: {1}'.format(m, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our model does a pretty good job of predicting total payment from number of claims. Obviously statistical models are never going to perfectly predict real world phenomena (especially simple linear models like the one we are using here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Great, so using ordinary least squares we calculated the optimal solution for m and b to best fit the dataset with our *y = mx + b* model. Before we try and do the same using gradient descent, let's talk more about what gradient descent is and how it works.\n",
    "\n",
    "Gradient descent is an optimization method. It is commonly used to optimize the values of variables in predictive models (in this example *m* and *b* from *y = mx + b*). It can be used for much more complex problems where there is not a nice easy formula to find an optimal solution (for example training neural networks).\n",
    "\n",
    "The basic steps of gradient descent are as follows.\n",
    "1. Start with random values for the variables you are trying to optimize\n",
    "2. Compute some form of error measurement for your predictive model using know pairs of input/output\n",
    "3. Use this error measurement to calculate a gradient for each variable\n",
    "4. Change each variable based on its gradient\n",
    "5. Repeat steps 2-4 until variables are (hopefully) optimized \n",
    "\n",
    "So what do I mean by gradient? Lets step back for a minute and talk about some calculus.\n",
    "\n",
    "![y = f(x) graph](res/ax1.jpg)\n",
    "\n",
    "Above is a graph of some arbitrary function *y = f(x)*. Let's say we are trying to find the value for *x* where *y* is minimum. Visually we can tell that it is around *x = 0*, and if we knew the function we could eaisly calculate the minimum mathematically. But what if we only knew a few points on the graph and the derivative of the function at those points? Could we still aproximate where y is minimum?\n",
    "\n",
    "![p1 graph](res/ax2.jpg)\n",
    "\n",
    "We can try! From the derivative of the function at point *p1*, we can tell that locally, reducing *x* will result in a reduction in *y*. In light of this new information let's say we take a new point *p2* that has a lower value of *x*.\n",
    "\n",
    "![p2 graph](res/ax3.jpg)\n",
    "\n",
    "Now we calculate the derivative at the point *p2*. And again we adjust our value of *x* based on the derivative. Lets say we keep following this strategy, we may eventually discover a value for $x$ that is very close to where *y* is minimum.\n",
    "\n",
    "![optimization complete graph](res/ax4.jpg)\n",
    "\n",
    "While this is a trivial example, it does illustrate the basic principle of how gradient descent optimization works. **Instead of optimizing x to find where y is minimum, gradient descent can also be used to optimize the variables of a predictive model to find where its 'mistakes' are minumum .** 'Mistakes' is usually expressed in terms of a **cost function**, which is some mathematical measure of a models performance chosen by the programmer. \n",
    "\n",
    "The strength of gradient descent is that this same method can still be applied to much more complex optimization problems with thousands of variables and very complex relationships between inputs and outputs (for example neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "We need to discuss another concept, and that is learning rate. The learning rate is a parameter that determines the magnitude of the change we make to the models variables based on the gradient (think of it as the distance between *p1* and *p2* in the graphs above). If the learning rate is too large, *m* and *b* will fluctuate wildly and our model will not converge towards an optimal solution. If learning rate is too small it will take far to long to converge towards an optimal solution.\n",
    "\n",
    "![too high learning rate](res/ax5.jpg)\n",
    "\n",
    "In the above graph the learning rate is too high, and the model does not converge towards an optimal solution\n",
    "\n",
    "![too low learning rate](res/ax6.jpg)\n",
    "\n",
    "In the above graph the learning rate is too low, and the model will take too long to converge towards an optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using gradient descent to solve our simple regression problem\n",
    "\n",
    "Ok, let's see if we can use the strategy outlined above to solve our linear regression problem. That is to find values for *m* and *b* that result in our predictive model (*y = mx + b*) being as accurate as possible. These are the steps we will take:\n",
    "\n",
    "1. Start with random values for m and b\n",
    "2. For a random input/output pair in the swedish auto dataset, calculate how well the model did (cost function)\n",
    "3. Calculate a gradient for m and a gradient for b with respect to the cost function\n",
    "4. Update the values of m and b based on this gradient\n",
    "5. Repeat steps 2-4\n",
    "\n",
    "Before we can dive right into the code we need to go over a few things.\n",
    "\n",
    "How exactly are we going to calculate a gradient for m and b based on how well the model performs?\n",
    "\n",
    "First we have to define a cost function. This is what we are going to try and minimize. \n",
    "\n",
    "$ C(m,b) = (a - y)^2 $\n",
    "\n",
    "Where *C* is our cost function, *m* and *b* are the variables we are trying to optimize, *a* is our models prediction and *y* is the ground truth value from our dataset.\n",
    "\n",
    "Now we need to calculate a gradient for both *m* and *b* with respect to this cost function.\n",
    "\n",
    "First lets expand the previous expression, substituting *mx + b* for *a*.\n",
    "\n",
    "$ C(m,b) = (mx + b - y)^2 $\n",
    "\n",
    "Now we want to compute a gradient for *m* and a gradient for *b* with respect to the cost function.\n",
    "\n",
    "Partial derivative with respect to *m* (applying chain rule)\n",
    "\n",
    "$ \\frac {\\partial C}{\\partial m} = 2(mx + b - y) * x $\n",
    "\n",
    "Partial derivative with respect to *b* (applying chain rule)\n",
    "\n",
    "$ \\frac {\\partial C}{\\partial b} = 2(mx + b - y)$\n",
    "\n",
    "If you are having trouble understanding the maths, I recommend you review partial differential equations (PDE's) and the chain rule.\n",
    "\n",
    "Now using these gradients we can determine how we want to change *m* and *b*. This change is multiplied by the learning rate. Because the change is multiplied by learning rate, we can ignore any constants (the leading 2 in this example).\n",
    "\n",
    "$ \\Delta m = (mx + b - y) * x * learning rate $\n",
    "\n",
    "$ \\Delta b = (mx + b - y) * learning rate $\n",
    "\n",
    "Ok now that we have a plan of attack, lets implement it in code...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a few helper functions. Their purpose is explained in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple data generator, yields individual input/output pairs from the dataset\n",
    "def simple_data_generator(x_dataset, y_dataset):\n",
    "    \n",
    "    assert x_dataset.shape[0] == y_dataset.shape[0]\n",
    "\n",
    "    num_items = x_dataset.shape[0]\n",
    "    index = 0\n",
    "\n",
    "    while True:\n",
    "        yield x_dataset[index], y_dataset[index]\n",
    "        index += 1\n",
    "        if index >= num_items:\n",
    "            index = 0\n",
    "            \n",
    "# normalizes a dataset. Assumes numpy array\n",
    "# normalization can have a range of meanings, in this context we are refering to feature scaling\n",
    "# scaling variables to a common scale will help our model train better\n",
    "def normalize_dataset(dataset, new_min, new_max):\n",
    "      \n",
    "    old_range = np.max(dataset) - np.min(dataset)\n",
    "    new_range = new_max - new_min\n",
    "\n",
    "    dataset *= (new_range / old_range)\n",
    "    dataset += (new_min - np.min(dataset))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# calculates the mean error of our predictive model over the entire dataset\n",
    "# this is not part of the training process, just evaluates the models performance\n",
    "def mean_error(x_dataset, y_dataset, m, b):\n",
    "    \n",
    "    total_error = sum([abs(y - (m * x + b)) for x, y in zip(x_dataset, y_dataset)])\n",
    "    return total_error / x_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set some things up in preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01 # choose a learning rate (how large steps to take at each training iteration)\n",
    "num_iterations = 4000 # choose how many times we repeat steps 2 - 4\n",
    "m = random.uniform(-1.0, 1.0) # initialize m and b to random values\n",
    "b = random.uniform(-1.0, 1.0)\n",
    "\n",
    "claims = normalize_dataset(claims, -1.0, 1.0) # scale both claims and payments to common scale (-1.0, 1.0)\n",
    "payments = normalize_dataset(payments, -1.0, 1.0)\n",
    "gen = simple_data_generator(claims, payments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent to optimize values of m and b (i.e. train the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "    x, y = next(gen) # get input/output pair from our generator\n",
    "\n",
    "    if i % 100 == 0:  # every 100 iterations print out an update on the training process\n",
    "        me = mean_error(claims, payments, m, b)\n",
    "        print('iteration: {0}, m: {1}, b: {2}, mean_error: {3}'.format(i, round(m, 4), round(b, 4), round(me, 4) ) )\n",
    "\n",
    "    del_m = (m * x + b - y) * x * learning_rate  # calculate gradient for m relative to cost, multiply by learning rate\n",
    "    del_b = (m * x + b - y) * learning_rate  # calculate gradient for b relative to cost, multiply by learning rate\n",
    "    \n",
    "    m -= del_m  # update m\n",
    "    b -= del_b  # update b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_pred = [m * x + b for x in claims]\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "plt.scatter(payments, claims, color='blue')\n",
    "plt.plot(gd_pred, claims, color='red')\n",
    "plt.title('Swedish auto insurance dataset\\nSimple linear regression')\n",
    "\n",
    "plt.xlabel('Number of claims (normalized)')\n",
    "plt.ylabel('Total payment for all claims (normalized)')\n",
    "plt.legend(['Gradient descent'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. We have found a (close to) optimal solution for our regression problem using gradient descent.\n",
    "\n",
    "This same strategy is used to train most modern machine learning algorithms (scaled up and with some clever improvements, but at its heart the same concept). Now that you understand how gradient descent works in this simple context you are ready for part two, where we apply the same principles to train a neural network to classify handwritten digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
